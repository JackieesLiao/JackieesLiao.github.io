{"posts":[{"title":"音视频开发之H.264","text":"H.264编码原理H.264，也被称为MPEG-4 AVC（Advanced Video Coding），是一种广泛使用的视频压缩标准，用于高清视频和网络视频传输。它采用先进的视频压缩技术，可以在保持较高视频质量的同时降低视频文件的大小，从而更有效地进行存储和传输。以下是关于H.264编码原理的基本介绍： 1. 帧类型和帧结构H.264编码器将视频流划分为一系列帧，包括关键帧（I帧）、预测帧（P帧）和双向预测帧（B帧）： 关键帧（I帧）：每隔若干帧，编码器会编码一帧完整的图像，不参考其他帧。这些帧通常用作视频的基础帧，不依赖于其他帧进行解码。 预测帧（P帧）：这些帧基于前面的关键帧或预测帧进行编码。P帧只存储了与前面帧不同的部分，通过运动补偿来描述图像变化。 双向预测帧（B帧）：这些帧利用前后帧的信息来编码，即引入了前向和后向预测，以进一步减少数据量。 2. 运动估计与补偿（Motion Estimation and Compensation）H.264编码器通过运动估计和运动补偿来寻找帧与帧之间的运动信息，从而减少冗余数据。运动估计是指预测当前帧与参考帧之间的位移或运动向量，而运动补偿则利用这些运动向量对当前帧进行预测。这种技术可以大大减少视频序列中的时间冗余，因为大部分帧在图像上只有微小的变化。 3. 变换与量化（Transform and Quantization）在H.264中，图像帧通过离散余弦变换（DCT）转换成频域，然后通过量化进一步减少数据量。DCT将图像数据从空间域转换为频域，使得图像中的能量可以更集中地表示在少量的频率系数上。量化过程将这些频率系数近似为较小的整数值，以减少所需的比特数。 4. 熵编码（Entropy Coding）最后，H.264使用熵编码技术（如CABAC和CAVLC）来进一步压缩数据。熵编码根据不同像素值的概率分布，分配更短的码字给出现频率高的像素值，以实现更高效的数据压缩。 5. 解码H.264解码器根据相同的原理进行反向操作：解码器接收压缩的视频数据流，包括帧间预测信息、运动向量、变换系数和熵编码数据。解码器首先对熵编码数据进行解码，然后进行逆量化、逆变换、运动补偿等操作，最终重建原始的视频帧。 H.264主要通过以上几种方法对数据进行压缩。 H.264算法特点 高压缩比： H.264采用先进的压缩技术，如运动估计、变换和熵编码等，可以实现比较高的压缩比。通过移除视频中的冗余信息和利用空间和时间上的相关性，H.264能够将视频数据压缩到相对较小的比特率下，同时保持较高的视频质量。 灵活的编码结构： H.264支持灵活的编码结构，可以根据应用场景和带宽条件进行调整。它允许选择不同的帧类型（I帧、P帧、B帧）以及帧间距离，从而在编码效率和延迟之间进行权衡。 运动估计与补偿： H.264通过运动估计和补偿技术来利用视频帧之间的相似性。运动估计通过寻找相邻帧之间的运动向量来描述对象的运动，而运动补偿则利用这些向量来预测当前帧的内容，从而减少冗余信息。 多帧并行处理： H.264支持多帧并行处理，允许编码器同时处理多个图像帧，以提高编码效率。这种并行处理技术可以利用现代处理器的多核心结构，加速编码过程。 自适应编码： H.264具有自适应编码功能，可以根据输入视频的特性和目标比特率进行动态调整。通过自适应的量化、帧间预测和熵编码策略，H.264可以在不同的场景下达到更好的编码效果。 错误恢复能力： H.264编码器具有一定的错误恢复能力，可以在数据丢失或错误的情况下仍能提供部分或完整的视频解码。这种特性对于视频传输和存储中的容错性非常重要。 H.264画质级别IPB帧 帧类型简介： I帧（Intra帧）：也称为关键帧，是视频序列中的重要帧，每个GOP（Group of Pictures，一组图像）的开始通常是一个I帧。I帧可以独立地解码，不依赖于其他帧。 P帧（Predicted帧）：P帧是通过对前向参考帧（通常是前面的I帧或P帧）进行运动估计和补偿来预测当前帧的内容。P帧存储了帧间预测所需的运动矢量和残差信息。 B帧（Bidirectional帧）：B帧使用前后两个参考帧（通常是前面的I帧或P帧，以及后面的P帧）进行运动估计和补偿。B帧可以更好地利用帧间的相关性，进一步减少视频数据量。 IPB帧的组织： 在视频序列中，连续的若干帧被组织成GOP（Group of Pictures）。一个GOP包含了一个I帧，后面是零个或多个P帧，以及零个或多个B帧。 典型的GOP结构可以是：I - (B - P) - (B - P) - … - (B - P)，其中括号内的表示一个B帧后跟一个P帧的组合。这种结构可以根据编码器的设置和视频内容进行调整。 IPB帧的作用： IPB帧的使用可以有效地减少视频数据量。P帧和B帧利用帧间的运动补偿技术，只存储帧间的差异信息，因此可以达到很高的压缩比。 P帧和B帧的引入使得视频编码更为灵活和高效，尤其在低比特率下能够提供较好的视觉质量和运动平滑性。 IPB帧的解码顺序： 当解码视频时，需要按照正确的顺序解码IPB帧，以确保视频帧按照时间顺序正确显示。通常解码顺序为：I帧 -&gt; P帧 -&gt; B帧。在解码B帧时，需要确保先解码其引用的前向和后向参考帧。 DTS与PTSDTS（Decode Time Stamp）和PTS（Presentation Time Stamp）是数字视频处理中常见的时间戳（timestamp）。 DTS（解码时间戳）： DTS 是指在视频解码器中的时间戳，表示视频帧被解码的时间。当视频流经过解码器解码时，每个视频帧都会被分配一个 DTS。DTS 表示视频帧实际被解码的时间点，即解码器开始解码该帧的时间。因为视频帧可能是按照不同的顺序传输或存储的（比如 B 帧可能依赖于其他帧），DTS 可用于正确地指示视频帧的解码顺序。 PTS（显示时间戳）： PTS 是指在视频播放器中的时间戳，表示视频帧应该被展示的时间。当视频播放器播放视频流时，每个视频帧都会被分配一个 PTS。PTS 表示视频帧应该在播放器中展示的时间点，即视频帧的实际显示时间。PTS 通常会根据视频的播放速度和时间轴来调整，以确保视频能够以正确的速率和顺序播放。 在视频处理中，DTS 和 PTS 通常用于控制视频的解码和播放顺序。解码器使用 DTS 来确保帧按照正确的顺序解码，而播放器则使用 PTS 来确保帧按照正确的时间显示和播放，从而实现流畅的视频播放体验。 H.264 NALUH.264（或称为 AVC，Advanced Video Coding）是一种常见的视频编码标准，其中的视频数据单元被称为 NALU（Network Abstraction Layer Unit，网络抽象层单元）。理解 H.264 的 NALU 对于理解视频压缩和传输过程中的数据组织和处理非常重要。 H.264 码流结构H.264 视频编码标准将视频数据流分解为多个 NALU，每个 NALU 包含了特定类型的数据，用于描述视频帧的各种信息和内容。NALU 是 H.264 数据的基本组成单元，也是在网络上传输和解码时的基本数据块。 H.264 的 NALU 类型在 H.264 中，每个 NALU 的类型由 NALU 头（NAL Unit Header）中的 NALU 类型字段指定。以下是常见的 H.264 NALU 类型： NALU 类型 1（非 IDR 图像的片段）： 这些 NALU 包含了 P 帧或 B 帧的数据。 NALU 类型 5（IDR 图像的片段）： 这些 NALU 包含了 I 帧（关键帧）的数据。IDR 帧是视频序列中的重要参考点，解码器可以从 IDR 帧开始解码视频序列。 NALU 类型 6（SEI，Supplemental Enhancement Information）： SEI NALU 包含了额外的辅助信息，例如时间戳、场景描述等。SEI NALU 可以用来增强视频数据的描述和传输。 NALU 类型 7（SPS，Sequence Parameter Set）： SPS NALU 包含了序列参数集，描述了视频序列的基本属性，如分辨率、帧率、图像采样格式等。 NALU 类型 8（PPS，Picture Parameter Set）： PPS NALU 包含了图像参数集，描述了当前图像的特定参数，如编码类型、参考帧索引等。 其他类型的 NALU： 还有其他类型的 NALU，用于描述不同类型的视频数据和辅助信息，例如解码器指令、填充数据等。 音频相关的参数 采样率：一秒钟的采样数，常见的44100，表示一秒钟的采样数据时44100个 位宽：采样位深 通道数 :一般为单通道或者双通道 码率 = 采样率（44.1k) * 位深度（16） *通道数（2） = 1411.2kbps 音频大小的文件 = 时长（300S）*码率（1411.2）/1024/8 = 51.67M 视频封装MP4的两种方式 需要编码 拿到的是RGB图片，写MP4，首先要编码成H264 不需要编码 MP4里面本质是H.264数据加AAC音频数据 第一种将图片编码成H.264,然后将264 AAC写成.mp4编码。","link":"/2024/05/11/H.264%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/05/11/hello-world/"},{"title":"音视频开发基础","text":"音视频开发一、音视频录制原理 二、音视频播放原理 三、图像基础图像基础知识 像素（Pixel）：图像的最小单位，每个像素对应图像中的一个点，具有特定的位置和颜色值。 分辨率（Resolution）：指图像中像素的密度，通常用像素数来表示，如1920x1080表示宽1920像素、高1080像素。 位深：看到的彩色图片都有三个通道分别为RGB（红绿蓝）。每个通道用8bit表示，8bit能表示256种颜色，可以组成256^3 = 1677万种颜色。这里的8bit就是位深。每个通道的位深越大，能够表示的颜色值越大。 灰度图像和彩色图像：灰度图像每个像素只有一个灰度值（亮度），彩色图像每个像素有红、绿、蓝三个颜色通道。 帧率：帧率即FPS(每秒有多少帧画面)。帧率越高，画面越流畅，需要的设备性能也越高。 码率：视频文件在单位事件内使用的数据流量。比如1Mbps。大多数情况下，码率越高则分辨率越高，也就越清晰。 YUVYUV 是一种颜色编码系统，通常用于数字视频和图像处理中。它将亮度（Luminance，Y）和色度（Chrominance，U和V）分离，有助于实现图像和视频的压缩和处理。 (A)组成部分： Y（亮度）： 表示图像的亮度信息，决定了像素的明暗程度。 Y 分量通常是灰度图像，代表图像的黑白信息。 U 和 V（色度）： U 和 V 分量描述了颜色信息，用于确定像素的色彩和饱和度。 U 分量表示蓝色与亮度之间的差值。 V 分量表示红色与亮度之间的差值。 (B)特点和用途： 色度分离：YUV 将亮度和色度分开处理，使得图像处理和压缩更有效。 视频压缩：许多视频编码标准（如 MPEG 和 H.264）使用 YUV 格式，通过对色度分量进行降采样和压缩来减少数据量。 节省带宽：对 U 和 V 分量的降采样可以减少数据传输所需的带宽，适用于视频流和存储。 数字图像处理：YUV 格式在图像处理中也有广泛应用，如颜色空间转换和特定图像处理算法。 (C)常见的 YUV 格式：在 YUV 格式中，存在两种常见的存储方式：打包格式（packed format）和平面格式（planar format）。这两种格式都是用于存储 YUV 图像数据的方式，但它们在数据组织和存储上有所不同。 1)打包格式（Packed Format）：在打包格式中，Y、U、V 数据按照一定顺序连续存储在内存中，形成一个紧密打包的数据块。这种方式使得每个像素的所有分量（亮度和色度）都存储在连续的内存区域中，适合于一些需要连续访问像素数据的场景，如视频渲染和处理。 常见的打包格式包括： YUYV/YUV422：每两个像素共用一对 UV 分量，数据排列为 Y0 U0 Y1 V0，每个分量占据一个字节或更多字节。 UYVY：类似于 YUYV，但顺序为 U0 Y0 V0 Y1。 优点： 内存占用较小，适合传输和处理。 访问速度快，连续的内存区域使得数据访问更有效率。 缺点： 数据分量不容易单独访问，需要解包操作。 2)平面格式（Planar Format）：在平面格式中，Y、U、V 数据分别存储在不同的内存区域（平面）中。即 Y 数据单独存储在一个内存块中，U 数据单独存储在另一个内存块中，V 数据也独立存储在另一个内存块中。这种方式使得每个像素的不同分量可以分开处理，更适合一些需要对分量进行独立处理的场景，如图像处理算法。 常见的平面格式包括： I420/YUV420：Y 数据存储在一个平面中，U 和 V 数据各自存储在独立的平面中。 NV12/NV21：Y 数据存储在一个平面中，U 和 V 数据交叉存储在另一个平面中。 优点： 分量独立存储，方便对单个分量进行处理和访问。 更容易实现图像处理算法，如颜色空间转换和滤波。 缺点： 内存占用相对较大，需要更多的内存空间来存储分量数据。 访问速度可能稍慢，因为需要分别访问多个内存区域。 例如： YUV420：最常见的 YUV 格式之一，包括 Y 分量和 U、V 分量的降采样版本。U 和 V 分量的采样率比 Y 分量低，通常是 YUV 中最常用的格式。表示2:1的水平下采样，2:1的垂直下采样，即每四个Y分量共用一个U分量和一个V分量。 YUV422：U 和 V 分量的水平采样率与 Y 分量相同，垂直采样率较低。表示2:1的水平下采样，没有垂直下采样，每两个Y分量共用一个U分量和一个V分量。 YUV444：每个像素都有自己的 U 和 V 分量，没有降采样。一个Y分量对应着一个U分量和一个V分量。 YUV与RGB互转RGB 到 YUV 转换公式：假设 RGB 值范围为 [0, 255]，YUV 值范围为 [0, 255]。 计算 Y（亮度）分量： 𝑌=0.299×𝑅+0.587×𝐺+0.114×𝐵Y=0.299×R+0.587×G+0.114×B 计算 U（蓝色色度）分量： 𝑈=−0.147×𝑅−0.289×𝐺+0.436×𝐵U=−0.147×R−0.289×G+0.436×B 计算 V（红色色度）分量： 𝑉=0.615×𝑅−0.515×𝐺−0.100×𝐵V=0.615×R−0.515×G−0.100×B YUV 到 RGB 转换公式：假设 YUV 值范围为 [0, 255]，RGB 值范围为 [0, 255]。 计算 R 分量： 𝑅=𝑌+1.140×𝑉R=Y+1.140×V 计算 G 分量： 𝐺=𝑌−0.395×𝑈−0.581×𝑉G=Y−0.395×U−0.581×V 计算 B 分量： 𝐵=𝑌+2.032×𝑈B=Y+2.032×U 在FFMPEG中使用如下代码进行转换： 1234567891011121314151617181920212223242526272829303132#include &lt;libavutil/imgutils.h&gt;#include &lt;libavformat/avformat.h&gt;// 将 RGB24 格式的图像数据转换为 YUV420p 格式void rgb_to_yuv(const uint8_t *rgb_data, uint8_t *yuv_data, int width, int height) { int src_linesize[1] = { 3 * width }; // RGB24 每行数据大小 int dst_linesize[3] = { width, width / 2, width / 2 }; // YUV420p 每行数据大小 // 分配 AVFrame 来存储 RGB 和 YUV 数据 AVFrame *src_frame = av_frame_alloc(); AVFrame *dst_frame = av_frame_alloc(); // 设置 RGB 数据到 src_frame av_image_fill_arrays(src_frame-&gt;data, src_frame-&gt;linesize, rgb_data, AV_PIX_FMT_RGB24, width, height, 1); // 设置 YUV 数据到 dst_frame av_image_fill_arrays(dst_frame-&gt;data, dst_frame-&gt;linesize, yuv_data, AV_PIX_FMT_YUV420P, width, height, 1); // 使用 sws_scale 进行颜色空间转换 struct SwsContext *sws_ctx = sws_getContext(width, height, AV_PIX_FMT_RGB24, width, height, AV_PIX_FMT_YUV420P, 0, NULL, NULL, NULL); sws_scale(sws_ctx, src_frame-&gt;data, src_frame-&gt;linesize, 0, height, dst_frame-&gt;data, dst_frame-&gt;linesize); // 释放资源 av_frame_free(&amp;src_frame); av_frame_free(&amp;dst_frame); sws_freeContext(sws_ctx);} 四、视频基础1. 视频码率（Bitrate）视频码率是指视频文件中每秒传输的数据量，通常以每秒传输的比特数（bps，比特每秒）来衡量。码率越高，视频的质量和清晰度就越高，但文件大小也会相应增加。常见的视频码率单位有千比特每秒（kbps）或兆比特每秒（Mbps）。 高码率 vs 低码率：高码率的视频通常具有更高的画质和细节，特别是对于快速运动的场景或高分辨率的视频。但高码率也会导致文件大小增加，不利于存储和传输。 2. 帧率（Frame Rate）帧率是指视频中每秒播放的图像帧数，通常以“帧每秒”（fps）来表示。帧率决定了视频的流畅度和动态效果，较高的帧率会使视频看起来更加流畅。常见的帧率包括 24fps、30fps、60fps 等。 流畅度 vs 资源消耗：增加帧率可以提高视频的流畅度，尤其是对于快速移动或动画内容。但较高的帧率会增加播放设备的性能要求，并可能导致文件大小增加。 3. 分辨率（Resolution）视频分辨率是指视频图像的像素尺寸，通常表示为宽度 x 高度（例如，1920x1080）。分辨率决定了视频的清晰度和细节水平，更高的分辨率意味着更多的像素，从而产生更清晰的图像。 高分辨率 vs 低分辨率：高分辨率视频具有更高的画质和细节，但也会占用更多的存储空间和带宽。常见的视频分辨率包括 720p（1280x720）、1080p（1920x1080）、4K（3840x2160）等。 4.I 帧I帧(Intra coded frames)I帧不需要参考其他画面而生成,解码时仅靠自己就重构完整图像。 I帧图像采用帧内编码方式: I帧所占数据的信息量比较大: I帧图像是周期性出现在图像序列中的，出现频率可由编码器选择帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各的质量);帧是帧组GOP的基础帧(第一帧),在一组中只有一个|帧; I帧不需要考虑运动矢量: 5.P帧P帧(Predicted frames):根据本帧与相邻的前一帧(1帧或P帧)的不同点来压缩本帧数据，同时利用了空间和时间上的相关性，》 P帧属于前向预测的帧间编码。它需要参考前面最靠近它的1帧或P帧来解码。 6.B帧B 帧(Bi-directional predicted frames):B帧图像采用双向时间预测，可以大大提高压缩倍数。 常用视频压缩算法H264、H265、AVS、VP8、VP9 五、音频基础1. 音频信号和波形 音频信号：声音是一种机械波，可以通过空气或其他介质传播。音频信号是声音的电子表示，通常是模拟信号或数字信号。 波形：音频波形描述了声音信号的振幅随时间的变化。常见的波形包括正弦波、方波、锯齿波等。 2. 音频参数 频率：音频信号的频率决定了声音的音调，单位为赫兹（Hz），通常人类可听范围是20 Hz到20,000 Hz（20 kHz）之间。 振幅：音频信号的振幅决定了声音的音量或响度，通常以分贝（dB）为单位表示。 3. 音频采样 采样：音频采样是将连续的模拟音频信号转换为离散的数字信号的过程。采样频率决定了每秒采集多少个样本，常见的采样频率有44.1 kHz、48 kHz等。 量化：音频信号的振幅值被量化为数字化的离散数值，通常用比特数表示（如16位、24位），决定了信号的动态范围和精度。 4. 音频编码和格式 编码：音频编码是将数字化的音频信号压缩和编码为文件，常见的音频编码格式包括MP3、AAC、FLAC等，用于存储和传输音频数据。 格式：音频格式定义了音频数据的存储方式和结构，常见的音频格式有WAV、AIFF、MP3、OGG等。 5. 音频处理 均衡：音频均衡调节不同频率部分的音量，以调整音频的音色和平衡。 混音：将多个音频信号混合在一起，以产生新的音频输出。 降噪：去除音频信号中的噪音和杂音，以提高音质。 6. 音频播放和传输 音频播放器：用于解码和播放音频文件的软件或硬件设备，例如Windows Media Player、iTunes、音频解码器等。 音频接口：用于传输音频信号的接口和协议，例如HDMI、USB音频接口、蓝牙等。 7.音频常见名词**帧:**每次编码的采样单元数，比如MP3通常是1152个采样点作为-个编码单元，AAC通常是1024个采样点作为一个编码单元。帧长:可以指每帧播放持续的时间:每帧持续时间(秒)=每帧采样点数/采样频率(HZ) 比如:MP3 48k,1152个采样点,每帧则为 24毫秒1152/48000=0.024秒=24毫秒;也可以指压缩后每帧的数据长度。 **交错模式:**数字音频信号存储的方式。数据以连续帧的方式存放，即首先记录帧1的左声道样本和右声道样本，再开始帧2的记录… **非交错模式:**首先记录的是一个周期内所有帧的左声道样本，再记录所有右声道样本。 ![屏幕截图 2024-04-24 202455](D:\\LearningPDF\\音视频\\屏幕截图 2024-04-24 202455.png) 8.音频编码原理数字音频信号如果不加压缩地直接进行传送，将会占用极大的带宽。例如，一套双声道数字音频若取样频率为44.1KHz，每样值按16bit量化，则其码率为:244.1kHz16bit=1.411Mbit/s如此大的带宽将给信号的传输和处理都带来许多困难和成本(阿里云服务器带宽大于5M后，每M价格是100元/月)，因此必须采取音频压缩技术对音频数据进行处理，才能有效地传输音频数据。数字音频压缩编码在保证信号在听觉方面不产生失真的前提下，对音频数据信号进行尽可能大的压缩，降低数据量。数字音频压缩编码采取去除声音信号中冗余成分的方法来实现。所谓余成分指的是音频中不能被人耳感知到的信号，它们对确定声音的音色，音调等信息没有任何的帮助。 冗余信号包含人耳听觉范围外的音频信号以及被掩蔽掉的音频信号等。例如，人耳所能察觉的声音信号的频率范围为20Hz~20KHz，除此之外的其它频率人耳无法察觉，都可视为冗余信号。此外，根据人耳听觉的生理和心理声学现象，当一个强音信号与-个弱音信号同时存在时，弱音信号将被强音信号所掩蔽而听不见这样弱音信号就可以视为冗余信号而不用传送。这就是人耳听觉的掩蔽效应，主要表现在频谱掩蔽效应和时域掩蔽效应。 9.封装格式封装格式(也叫容器)就是将已经编码压缩好的视频流、音频流及字幕按照一定的方案放到一个文件中，便于播放软件播放，一般来说，视频文件的后缀名就是它的封装格式。封装的格式不一样，后缀名也就不一样(比如.mp4 .flv)。 常见的视频封装格式有： 1234//AVI、MKV、MPE、MPG、MPEGMP4、WMV、MOV、3GPM2V、M1V、M4V、OGMRM、RMS、RMM、RMVB、IFO//SWF、FLV、F4V、//ASF、PMF、XMB、DIVX、PART//DAT、VOB、M2TS、TS、PS H264+AAC封装为FLV或MP4是最为流行的模式 10.音视频同步DTS(Decodirg Time Stamp)即解码时间戳，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。 PTS(Presentation Time Stamp):即显示时间戳，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。 Audio Master: 同步视频到音频 Video Master:同步音频到视频 External Clock Master:同步音频和视频到外部时钟 六、搭建Linux下的FFMPEG编译环境1.首先分别创建三个文件夹用于下载源文件以及存储编译后的库文件和二进制文件。1mkdir ffmpeg_sources ffmpeg_build bin 2.安装ffmpeg的依赖：123456789sudo apt-get install -y \\autoconf automake build-essential git \\libass-dev libfreetype6-dev libsdl2-dev \\libtheora-dev libtool libva-dev \\libvdpau-dev libvorbis-dev libxcb1-dev \\libxcb-shm0-dev libxcb-xfixes0-dev \\pkg-config texinfo wget zlib1g-dev \\libavformat-dev libavcodec-dev libswresample-dev libswscale-dev libavutil-dev libsdl1.2-dev yasm libsdl2-dev \\libx264-dev libx265-dev libfdk-aac-dev 3.输入如下命令行进行编译安装：12345678910111213PATH=&quot;$HOME/bin:$PATH&quot; PKG_CONFIG_PATH=&quot;$HOME/ffmpeg_build/lib/pkgconfig&quot; ./configure \\&gt; --prefix=&quot;$HOME/ffmpeg_build&quot; \\&gt; --pkg-config-flags=&quot;--static&quot; \\&gt; --extra-cflags=&quot;-I$HOME/ffmpeg_build/include&quot; \\&gt; --extra-ldflags=&quot;-L$HOME/ffmpeg_build/lib&quot; \\&gt; --extra-libs=&quot;-lpthread -lm&quot; \\&gt; --bindir=&quot;$HOME/bin&quot; \\&gt; --enable-gpl \\&gt; --enable-libass \\&gt; --enable-libx264 \\&gt; --enable-libx265 &amp;&amp; PATH=&quot;$HOME/bin:$PATH&quot; make &amp;&amp; \\&gt; make install &amp;&amp; \\&gt; hash -r 4.将生成的build目录下的bin文件夹里的可执行文件加入环境变量：1export PATH=/home/jeanxliao/ffmpeg_build/bin/:$PATH 5.验证是否安装成功：1ffmpeg -version","link":"/2024/05/11/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"音视频开发","slug":"音视频开发","link":"/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91/"},{"name":"音视频开发H.264","slug":"音视频开发H-264","link":"/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91H-264/"},{"name":"音视频开发基础","slug":"音视频开发基础","link":"/tags/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80/"}],"categories":[{"name":"音视频开发","slug":"音视频开发","link":"/categories/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91/"},{"name":"音视频开发H.264","slug":"音视频开发/音视频开发H-264","link":"/categories/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91H-264/"},{"name":"音视频开发基础","slug":"音视频开发/音视频开发基础","link":"/categories/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91/%E9%9F%B3%E8%A7%86%E9%A2%91%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80/"}],"pages":[]}